{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Rafael Garcia Cano da Costa\n",
    "NetID: rgc292\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Case study\n",
    "- Read [this article](http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html) in the New York Times\n",
    "- Use what we've learned in class as well as from the book and our readings to describe how one could set this up as a predictive modeling problem, such that they could have gotten the results that they did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having in mind that the collection of customers' data offers the chance to better understand clients' wants and needs, and that the existence of a great change in people's lives is an opportunity to guide them towards new habits, Target have a good insight to increase its sales through gathering new customers to buy more items within its stores when experiencing the arrival of a new born. To accomplish this, the then current dataset containing clients' purchases could be used as a good start, but a continuous observation and colection of data referent to new purchases would be important in order to update the already established dataset. Having this done, the better approach would be the use of a supervised learning on a regression instance because the target attribute is known, which is the likelihood of a woman buyer be pregnant. Keeping this in mind, the exploration and cleaning of the data should be performed as a means to identify the best attributes to characterize the common purchase behavior a pregnant women presents. Therefore, labeling the ones offering the higher information gain would be valuable. After that, the decision tree would be a great predictive model to build a multivariate supervised segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Exploring data in the command line\n",
    "For this part we will be using the data file located in `\"data/advertising_events.csv\"`. This file consists of records that pertain to some advertising events on a given day. There are 4 comma separated columns in this order: `userid`, `timestamp`, `domain`, and `action`. These fields are of type `int`, `int`, `string`, and `int` respectively. Answer the following questions using Linux/Unix bash commands. All questions can be answered in one line (sometimes, with pipes)! Some questions will have many possible solutions. Don't forget that in IPython notebooks you must prefix all bash commands with an exclamation point, i.e. `\"!command arguments\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. How many records are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wc -l advertising_events.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many unique users are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cut -d \",\" -f 1 advertising_events.csv | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Rank all domains by the number of visits they received in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cut -d \",\" -f 3 advertising_events.csv | sort | uniq -c | sort -n -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. List all records for the user with user id 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!awk -F\",\" '$1 == 37 { print $1\" \"$2\" \"$3\" \"$4 }' advertising_events.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Dealing with messy data\n",
    "Not all data you will deal with is going to be clean. In fact, much of it will be very messy! For example, we have the HTML page that lists the contributors to Facebook's [osquery](https://github.com/facebook/osquery) project that is hosted on [Github.com](https://github.com). In this case, all we are interested in are the contributors and how many commits each of them has. Given the HTML page in `\"data/osquery_contributors.html\"` you will sift through tons of irrelevant data so that you can build a useful data structure.\n",
    "\n",
    "Notice that the first six (out of 59 total) contributors are named \"theopolis\", \"marpaia\", \"javuto\", \"jedi22\", \"unixist\", and \"mofarrell\". They have 553, 477, 104, 49, 30, 25 commits respectively.\n",
    "\n",
    "![Screenshot](images/osquery_contributors.png)\n",
    "\n",
    "To get a better of understanding of how this data is stored in the file, try searching through the raw data file for these usernames to look for any patterns. Your final dictionary should have 59 elements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Turn this data into a Python dictionary called `contributors` where the keys are the contributor names and the values are the number of commits that each contributor has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributors = dict()\n",
    "# Place your code here\n",
    "!awk '/author=/' osquery_contributors.html | cut -d \"=\" -f 4 | cut -d '\"' -f 1 > names.txt\n",
    "!awk '/author=/' osquery_contributors.html | cut -d \"=\" -f 4 | cut -d '>' -f 2 | cut -d ' ' -f 1 > values.txt\n",
    "!paste -d\",\" names.txt values.txt > dictpair.txt\n",
    "\n",
    "file = open('dictpair.txt')\n",
    "for line in file:\n",
    "    x = line.split(\",\")    \n",
    "    a=x[0]\n",
    "    b=x[1].split(\"\\n\")[0]\n",
    "    contributors[a]=b\n",
    "# This line will print your dictionary for grading purposed. Do not remove this line!!!    \n",
    "print contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Dealing with data Pythonically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might find these packages useful. You may import any other you want!\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Download the data set `\"data/ads_dataset.tsv\"` and load it into a Python Pandas data frame called `ads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "ads = DataFrame.from_csv(\"ads_dataset.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Write a Python function called `getDfSummary()` that does the following:\n",
    "- Takes as input a data frame\n",
    "- For each variable in the data frame calculates the following features:\n",
    "  - `number_nan` to count the number of missing not-a-number values\n",
    "  - Ignoring missing, NA, and Null values:\n",
    "    - `number_distinct` to count the number of distinct values a variable can take on\n",
    "    - `mean`, `max`, `min`, `std` (standard deviation), and `25%`, `50%`, `75%` to correspond to the appropriate percentiles\n",
    "- All of these new features should be loaded in a new data frame. Each row of the data frame should be a variable from the input data frame, and the columns should be the new summary features.\n",
    "- Returns this new data frame containing all of the summary information\n",
    "\n",
    "Hint: The pandas `describe()` method returns a useful series of values that can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def getDfSummary(ads):\n",
    "def getDfSummary(file):\n",
    "# Place your code here\n",
    "  number_nan = pd.DataFrame(file.isnull().sum(), columns=['number_nan'])\n",
    "  description = pd.DataFrame(file.describe().T) \n",
    "  description_change  = description.rename(columns = {'count':'number_distinct'}) \n",
    "  union = number_nan.join(description_change)\n",
    "  output_data = union\n",
    "#  return output_data    \n",
    "  return output_data\n",
    "\n",
    "getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. How long does it take for your `getDfSummary()` function to work on your `ads` data frame? Show us the results below.\n",
    "\n",
    "Hint: `%timeit getDfSummary(ads)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the results returned from `getDfSummary()`, which fields, if any, contain missing `NaN` values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getDfSummary(ads)\n",
    "df[df['number_nan'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. For the fields with missing values, does it look like the data is missing at random? Are there any other fields that correlate perfectly, or predict that the data is missing? If missing, what should the data value be?\n",
    "\n",
    "Hint: create another data frame that has just the records with a missing value. Get a summary of this data frame using `getDfSummary()` and compare the differences. Do some feature distributions change dramatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_df = pd.DataFrame(ads[ads['buy_freq'] != 1])\n",
    "getDfSummary(missing_df)\n",
    "\n",
    "# It does not look that the data is missing. The fact that the respective value for \"isbuyer\" is zero explains the fact \n",
    "# that there is no buy_freq. If it was mising, the data should be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Which variables are binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Although (dtypes) shows that no one of the variables are binary, the variable (isbuyer) seems to be used as a boolean: \n",
    "  # 1 as yes and 0 as no \n",
    "# no one of them\n",
    "df1 = getDfSummary(ads).T\n",
    "df1.dtypes\n",
    "# no one of them\n",
    "df2 = getDfSummary(ads)\n",
    "df2.dtypes\n",
    "# no one of them\n",
    "ads.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Generate a correlation matrix for the `ads` data frame. Is there any redundancy in the data? Are there any features that aren't needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads.corr()\n",
    "#Yes. (isbuyer and buy_freq) and (last_buy and last_visit) can be considered a redundancy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
